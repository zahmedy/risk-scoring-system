Metadata-Version: 2.4
Name: risk-scoring-system
Version: 0.1.0
Summary: A risk scoring system for evaluating and managing risk assessments
Author-email: Zayed Ahmed <zayed.ahmd@gmail.com>
License: MIT
Keywords: risk,scoring,assessment
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: black>=22.0; extra == "dev"
Requires-Dist: flake8>=4.0; extra == "dev"
Requires-Dist: mypy>=0.950; extra == "dev"

# Risk Scoring System
End-to-end credit risk scoring pipeline that goes from raw tabular data to trained model, evaluation, and a FastAPI-ready serving skeleton.

## What you get
- **Clear problem framing**: Binary credit risk classification with configurable target/positive label and train/test split in `configs/base.yaml`.
- **Reproducible pipeline**: Config-driven preprocessing (impute/encode/scale), Random Forest baseline, and saved artifacts (`model.joblib`, `preprocessor.joblib`, `schema.json`, `baseline_stats.json`, `metrics.json`).
- **Decision-ready evaluation**: Threshold search for cost-sensitive decisions plus standard metrics and confusion matrices saved under `artifacts/`.
- **Operational guardrails**: Baseline feature distributions captured for future drift checks; monitoring config stubbed for expansion.
- **Serv-ing hook**: FastAPI scaffold in `api/` ready to load the trained artifacts for `/predict`.

## Repository tour
```
risk-scoring-system/
├─ configs/            # data, model, monitor, and policy configs
├─ data/               # raw/processed CSVs (gitignored locally)
├─ artifacts/          # trained model, preprocessor, schema, metrics
├─ notebooks/          # EDA, baselines, threshold tuning, drift checks
├─ src/risk_system/    # pipeline code (data, preprocess, train, eval)
├─ api/                # FastAPI entrypoint (connect artifacts to serve)
└─ test/               # unit tests
```

## Quickstart
1) **Environment**  
   ```bash
   python -m venv .venv && source .venv/bin/activate
   pip install pandas numpy scikit-learn pyyaml joblib fastapi uvicorn pytest
   ```
2) **Data**  
   Place your CSV at `data/raw/risk.csv` (path is configurable in `configs/base.yaml`), and set `target` and `positive_label` there.
3) **Train**  
   ```bash
   python -m risk_system.cli train --base configs/base.yaml --model configs/model.yaml --artifact-dir artifacts
   ```
   This fits the model and writes artifacts/metrics.
4) **Evaluate with policy/thresholds**  
   ```bash
   python -m risk_system.cli evaluate --base configs/base.yaml --policy configs/policy.yaml --artifact-dir artifacts
   ```
5) **Explore notebooks**  
   ```bash
   jupyter notebook notebooks/
   ```

## Baseline snapshot (included artifacts)
- Random Forest baseline on the sample data: `accuracy=0.785`, `roc_auc=0.785`, `pr_auc=0.648` (`artifacts/metrics.json`).
- Cost-sensitive thresholding example: `threshold=0.31` with `accuracy=0.851`, `precision=0.681`, `recall=0.947`, `roc_auc=0.959`, `pr_auc=0.926` (`artifacts/eval_metrics.json`).
- Confusion matrix at the tuned threshold: TN=567, FP=133, FN=16, TP=284.
- `baseline_stats.json` stores feature distribution snapshots to compare future batches for drift.

## Workflow at a glance
1) Configure `configs/base.yaml` (data paths/target) and `configs/model.yaml` (RF hyperparameters).
2) Run `risk_system.cli train` to fit and materialize artifacts to `artifacts/`.
3) Run `risk_system.cli evaluate` with `configs/policy.yaml` to pick decision thresholds by cost/recall.
4) Iterate in notebooks (`02_baseline_lr`, `03_rf_comparison`, `04_threshold_costs`, `05_drift_checks`) to compare models and monitor stability.
5) Wire `api/main.py` to load `preprocessor.joblib` + `model.joblib` for online scoring.

## Roadmap ideas
- Add dependency pins to `pyproject.toml` and publish to PyPI for clean installs.
- Persist monitoring outputs (drift scores, alert thresholds) and automate with a scheduled job.
- Expand evaluation to include calibration plots and fairness slices.
- Harden the FastAPI layer with request/response schemas, input validation, and batch scoring endpoint.
